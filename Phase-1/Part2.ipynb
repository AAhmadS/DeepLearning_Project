{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikelroid/DeepLearning_Project/blob/main/Phase-1/Part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJIuWR9iQiuZ"
      },
      "source": [
        "#Loading data onto the disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znR9f5HDHZEA",
        "outputId": "95808d5d-f8b0-49f5-b4ec-f2228f0acc82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAYPyv5YRhmU",
        "outputId": "1f1e1ad7-9052-47fc-aed3-08ce78f55eb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deep_learning/Project\n"
          ]
        }
      ],
      "source": [
        "%cd drive/My Drive/deep_learning/Project"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run this just for the first time**"
      ],
      "metadata": {
        "id": "Z3vmxmR_9AvR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "src-bECn8HMF",
        "outputId": "0ca016d7-0eb7-40cf-d47a-d055e5a4ac50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: 'train_ende.zip' and './train_ende.zip' are the same file\n",
            "cp: cannot stat 'test.zip': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!cp train_ende.zip .\n",
        "!cp test.zip .\n",
        "! git clone https://github.com/XL2248/MSCTD\n",
        "!cp MSCTD/MSCTD_data/ende/english_*.txt .\n",
        "!cp MSCTD/MSCTD_data/ende/image_index_*.txt .\n",
        "!cp MSCTD/MSCTD_data/ende/sentiment_*.txt .\n",
        "# !pip install --upgrade --no-cache-dir gdown\n",
        "# !gdown --id 1GAZgPpTUBSfhne-Tp0GDkvSHuq6EMMbj\n",
        "# !gdown --id 1B9ZFmSTqfTMaqJ15nQDrRNLqBvo-B39W\n",
        "%%bash\n",
        "for x in dataset/*.zip\n",
        "do\n",
        "  unzip -qq $x\n",
        "done;\n",
        "!mkdir dataset\n",
        "!cd dataset; mkdir train test dev\n",
        "!mv *train* dataset/train\n",
        "!mv *test* dataset/test\n",
        "!mv *dev* dataset/dev"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsNo89PVQoTr"
      },
      "source": [
        "#Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opbYa_BsJgpH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms as T\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "from torchvision.models import efficientnet_b2, EfficientNet_B2_Weights\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from itertools import groupby\n",
        "import seaborn as sns\n",
        "# import linecache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk066Jt1J9hg"
      },
      "outputs": [],
      "source": [
        "class MSCTD_Dataset (Dataset):\n",
        "  def __init__(self, dataset_dir, images_dir, conversation_dir, texts, sentiments, transform):\n",
        "    self.dataset_path = Path(dataset_dir)\n",
        "    self.images_path = self.dataset_path / images_dir\n",
        "    self.sentiment_path = self.dataset_path / sentiments\n",
        "    self.text_path = self.dataset_path / texts\n",
        "    self.conversations_path = self.dataset_path / conversation_dir\n",
        "    self.transform = transform\n",
        "\n",
        "    with open(self.sentiment_path, 'r') as f:\n",
        "      self.length = len(f.readlines())\n",
        "\n",
        "    with open(self.text_path, 'r') as f:\n",
        "        self.texts = f.read().splitlines()\n",
        "\n",
        "    with open(self.sentiment_path, 'r') as f:\n",
        "        self.sentiments = np.array(f.read().splitlines()).astype(\"int32\")\n",
        "    \n",
        "    with open(self.conversations_path, 'r') as f:\n",
        "        self.conversations = np.array(f.read().splitlines())\n",
        "    \n",
        "  def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        img_path = self.images_path / f'{idx}.jpg'\n",
        "        image = np.divide(np.array(Image.open(img_path)),255)\n",
        "\n",
        "        # image = read_image(str(img_path))\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "       \n",
        "        txt = self.texts[idx].strip()\n",
        "        \n",
        "        sentiment = self.sentiments[idx]\n",
        "\n",
        "        data_dict = {\"text\":txt,\n",
        "                     \"image\":image,\n",
        "                     \"sentiment\":sentiment}\n",
        "        return image,sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgpzBuvVzV8h"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()\n",
        "                                ,transforms.Resize((288,288),transforms.InterpolationMode(\"bicubic\"))\n",
        "                                ,transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n",
        "\n",
        "trainset = MSCTD_Dataset('dataset/train', 'train_ende', 'image_index_train.txt', 'english_train.txt', 'sentiment_train.txt',transform)\n",
        "testset = MSCTD_Dataset('dataset/test', 'test', 'image_index_test.txt', 'english_test.txt', 'sentiment_test.txt',transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pp_at-RNMX7B"
      },
      "outputs": [],
      "source": [
        "image, sentiment = testset[10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2NXwODzu0cP"
      },
      "outputs": [],
      "source": [
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXhRO8HviZrT",
        "outputId": "7e664e87-c675-44b9-8aeb-d6f66d4bc0c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda for inference\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f'Using {device} for inference')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fINqwWLmjP2"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "data_dir = './data'\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhtmT-QBZlVW"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "def train_epoch(net: nn.Module, criterion: nn.Module, optimizer: torch.optim.Optimizer, dataloader: torch.utils.data.DataLoader,   accs_train ,loss_train):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_true = 0\n",
        "    epoch_all = 0\n",
        "    i = 0\n",
        "\n",
        "    net.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    with tqdm(enumerate(dataloader), total=len(dataloader)) as pbar:\n",
        "        for i, (x, y) in pbar: \n",
        "            x = x.to(device).float()\n",
        "            y = y.to(device).to(torch.int64)\n",
        "            \n",
        "            p = net(x).float()\n",
        "            loss = criterion(p, y)\n",
        "            epoch_loss += float(loss)\n",
        "            predictions = p.argmax(-1)\n",
        "            epoch_all += len(predictions)\n",
        "            epoch_true += (predictions == y).sum()\n",
        "            pbar.set_description(f'Loss: {epoch_loss / (i + 1):.3e} - Acc: {epoch_true * 100. / epoch_all:.2f}%')\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "          \n",
        "        accs_train.append(float(epoch_true / epoch_all))\n",
        "        loss_train.append(float(epoch_loss / (i + 1)))\n",
        "    return accs_train,loss_train\n",
        "\n",
        "def eval_epoch(net: nn.Module, criterion: nn.Module, dataloader: torch.utils.data.DataLoader,    accs_test ,loss_test ):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_true = 0\n",
        "    epoch_true_topfive = 0\n",
        "    epoch_all = 0\n",
        "    i = 0\n",
        "\n",
        "    net.eval()\n",
        "    with torch.no_grad(), tqdm(enumerate(dataloader), total=len(dataloader)) as pbar:\n",
        "        for i, (x,y) in pbar:\n",
        "            \n",
        "            x = x.to(device).float()\n",
        "            y = y.to(device).to(torch.int64)\n",
        "            p = net(x).float()\n",
        "            loss = criterion(p, y)\n",
        "            epoch_loss += float(loss)\n",
        "\n",
        "            # predict \n",
        "            predictions = p.argmax(-1)\n",
        "            epoch_all += len(predictions)\n",
        "            epoch_true += (predictions == y).sum()\n",
        "\n",
        "            pbar.set_description(f'Loss: {epoch_loss / (i + 1):.3e} - Acc: {epoch_true * 100. / epoch_all:.2f}% ')\n",
        "\n",
        "        accs_test.append(float(epoch_true / epoch_all))\n",
        "        loss_test.append(float(epoch_loss / (i + 1)))\n",
        "    return accs_test,loss_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzNXevTTtnR8",
        "outputId": "f4c60fcb-9c73-4308-c85b-f65c45535506"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Params to learn:\n",
            "\t last.1.weight\n",
            "\t last.1.bias\n",
            "\t last.3.weight\n",
            "\t last.3.bias\n",
            "\t last.5.weight\n",
            "\t last.5.bias\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.088e+00 - Acc: 39.13%: 100%|██████████| 317/317 [1:22:54<00:00, 15.69s/it]\n",
            "Loss: 1.093e+00 - Acc: 40.71% : 100%|██████████| 80/80 [34:45<00:00, 26.07s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1 finished in 7060.13s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.080e+00 - Acc: 40.72%: 100%|██████████| 317/317 [08:33<00:00,  1.62s/it]\n",
            "Loss: 1.096e+00 - Acc: 39.04% : 100%|██████████| 80/80 [01:56<00:00,  1.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   2 finished in 630.07s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 1.074e+00 - Acc: 41.82%: 100%|██████████| 317/317 [08:24<00:00,  1.59s/it]\n",
            "Loss: 1.100e+00 - Acc: 36.33% : 100%|██████████| 80/80 [01:56<00:00,  1.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   3 finished in 620.97s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.073e+00 - Acc: 42.08%: 100%|██████████| 317/317 [08:32<00:00,  1.62s/it]\n",
            "Loss: 1.099e+00 - Acc: 39.75% : 100%|██████████| 80/80 [01:58<00:00,  1.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   4 finished in 631.22s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 1.069e+00 - Acc: 42.50%: 100%|██████████| 317/317 [08:37<00:00,  1.63s/it]\n",
            "Loss: 1.097e+00 - Acc: 40.32% : 100%|██████████| 80/80 [01:58<00:00,  1.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   5 finished in 635.35s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 1.068e+00 - Acc: 42.82%: 100%|██████████| 317/317 [08:36<00:00,  1.63s/it]\n",
            "Loss: 1.102e+00 - Acc: 39.16% : 100%|██████████| 80/80 [01:58<00:00,  1.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   6 finished in 634.83s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 1.066e+00 - Acc: 42.87%: 100%|██████████| 317/317 [08:33<00:00,  1.62s/it]\n",
            "Loss: 1.111e+00 - Acc: 37.25% :  62%|██████▎   | 50/80 [01:13<00:40,  1.35s/it]"
          ]
        }
      ],
      "source": [
        "class lastLayer(nn.Module):\n",
        "    def __init__(self, pretrained):\n",
        "        super(lastLayer, self).__init__()\n",
        "        self.pretrained = pretrained\n",
        "        self.last = nn.Sequential(\n",
        "            nn.Dropout(p = 0.2,inplace=True),\n",
        "            nn.Linear(1408, 90),\n",
        "            nn.Dropout(p = 0.3,inplace=True),\n",
        "            nn.Linear(90, 30),\n",
        "            nn.Dropout(p = 0.1,inplace=True),\n",
        "            nn.Linear(30, 3),\n",
        "            )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.pretrained(x)\n",
        "        x = self.last(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "net = efficientnet_b2(weights=EfficientNet_B2_Weights.IMAGENET1K_V1)\n",
        "net.classifier = nn.Sequential()\n",
        "\n",
        "for param in net.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "net = lastLayer(net).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "print(\"Params to learn:\")\n",
        "params_to_update = []\n",
        "for name,param in net.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        params_to_update.append(param)\n",
        "        print(\"\\t\",name)\n",
        "\n",
        "optimizer = torch.optim.RMSprop(params_to_update, lr=2e-4)\n",
        "\n",
        "epochs = 20\n",
        "from time import time\n",
        "accs_train = []\n",
        "loss_train = []\n",
        "accs_test = []\n",
        "loss_test = []\n",
        "\n",
        "\n",
        "for e in range(epochs):\n",
        "    start_time = time()\n",
        "    accs_train,loss_train = train_epoch(net, criterion, optimizer, train_loader,accs_train,loss_train)\n",
        "    accs_test,loss_test = eval_epoch(net, criterion, test_loader,accs_test,loss_test)\n",
        "    if accs_test[-1]==max(accs_test):\n",
        "      torch.save(net.state_dict(), 'scene_modal_en.pth')\n",
        "    end_time = time()\n",
        "\n",
        "    print(f'Epoch {e+1:3} finished in {end_time - start_time:.2f}s')\n",
        "\n",
        "plt.plot(np.array(loss_test), 'r')\n",
        "plt.plot(np.array(loss_train), 'b')\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Test', 'Train'])\n",
        "plt.savefig('loss4.jpg')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.array(accs_test), 'r')\n",
        "plt.plot(np.array(accs_train), 'b')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Test', 'Train'])\n",
        "plt.savefig('acc4.jpg')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(f'Best Accuracy :{max(accs_test) * 100.:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x4aRsYaznWWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 quistion"
      ],
      "metadata": {
        "id": "9fzth1A0LOH-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sumf8Rt5XzWz"
      },
      "source": [
        "\n",
        "No, This manner is not suitable for this task. It is because emotions and feelings in movies are related to the faces, the mood of voices, and conversations. Scan modes and emotions of scenes in the movies without considering characters' faces and conditions have not very good results in this task.</br>\n",
        "This accuracy (43%) is more than base accuracy (33%) just because of the general situation and some parameters of an image like brightness and angle of the camera detected and trained on them.\n",
        "This model uses EfficientNet_B2 with about 8M parameters and three layers with 90, 30, and 10 neurons.</br>\n",
        "In the training process, after epoch 2, we have a strong overfit on the training dataset, so by intensely decreasing the accuracy of validation, we can say that we can have a better result in this model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}