{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikelroid/DeepLearning_Project/blob/main/Phase-3/Phase3_Part2_debugged.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YW9PBIcfrEb9"
      },
      "outputs": [],
      "source": [
        "# https://drive.google.com/file/d/1blGgEOlrHrM0-NAQxYVRwMlfiHDvVHXb/view\n",
        "#!pip install --upgrade --no-cache-dir gdown\n",
        "#!gdown --id \"1blGgEOlrHrM0-NAQxYVRwMlfiHDvVHXb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yHl0cqOklIr"
      },
      "outputs": [],
      "source": [
        "#!unzip InstaNY100K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YoIG770-WmH"
      },
      "outputs": [],
      "source": [
        "#!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNwkS_vw_zQY"
      },
      "outputs": [],
      "source": [
        "#import json \n",
        "#\n",
        "#api = {\"username\":\"a80abbasi\",\"key\":\"c64917f1d1c750fbbeed83ac15b782e8\"}\n",
        "#with open('kaggle.json', 'w') as f:\n",
        "#    json.dump(api, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0WPeUjP_v8b"
      },
      "outputs": [],
      "source": [
        "#!mkdir ~/.kaggle\n",
        "#!cp kaggle.json ~/.kaggle/\n",
        "#!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8L6OKvOgAWNn"
      },
      "outputs": [],
      "source": [
        "#!kaggle datasets download -d hsankesara/flickr-image-dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "M9ygWHV3B_NI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bd932f1-dc12-41c5-aa21-74f659fa14ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp drive/MyDrive/flickr-image-dataset.zip /"
      ],
      "metadata": {
        "id": "adS6yuY6B5sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guMAw9uWBwmA"
      },
      "outputs": [],
      "source": [
        "#!unzip flickr-image-dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFpW6boyzsiF"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHxFDfhwzQsT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fTD7sQI0Vlm"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gPqx94iCYjQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "a8e59f4f-d761-41d4-a39e-0f9b6819a118"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       image_name  comment_number  \\\n",
              "0  1000092795.jpg               0   \n",
              "1  1000092795.jpg               1   \n",
              "2  1000092795.jpg               2   \n",
              "3  1000092795.jpg               3   \n",
              "4  1000092795.jpg               4   \n",
              "\n",
              "                                             comment  \n",
              "0   Two young guys with shaggy hair look at their...  \n",
              "1   Two young , White males are outside near many...  \n",
              "2   Two men in green shirts are standing in a yard .  \n",
              "3       A man in a blue shirt standing in a garden .  \n",
              "4            Two friends enjoy time spent together .  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-68b3edb8-7d67-4a48-8a45-535f3a8f7cc9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>comment_number</th>\n",
              "      <th>comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>Two young guys with shaggy hair look at their...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>Two young , White males are outside near many...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>2</td>\n",
              "      <td>Two men in green shirts are standing in a yard .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>3</td>\n",
              "      <td>A man in a blue shirt standing in a garden .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>Two friends enjoy time spent together .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-68b3edb8-7d67-4a48-8a45-535f3a8f7cc9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-68b3edb8-7d67-4a48-8a45-535f3a8f7cc9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-68b3edb8-7d67-4a48-8a45-535f3a8f7cc9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "df = pd.read_csv('./flickr30k_images/results.csv', delimiter='|')\n",
        "df = df.dropna()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1B3EwLtDtgo"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# general config\n",
        "MAX_LEN = 30\n",
        "\n",
        "TRAIN_BATCH_SIZE = 64\n",
        "VALID_BATCH_SIZE = 64\n",
        "TEST_BATCH_SIZE = 64\n",
        "\n",
        "EPOCHS = 4\n",
        "LEARNING_RATE = 5e-5\n",
        "\n",
        "MODEL_NAME = 'bert-base-uncased'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk066Jt1J9hg"
      },
      "outputs": [],
      "source": [
        "class MSCTD_Dataset (Dataset):\n",
        "  def __init__(self, dataset_dir, images_dir, conversation_dir, texts, sentiments, transform=None):\n",
        "    self.dataset_path = Path(dataset_dir)\n",
        "    self.images_path = self.dataset_path / images_dir\n",
        "    self.sentiment_path = self.dataset_path / sentiments\n",
        "    self.text_path = self.dataset_path / texts\n",
        "    self.conversations_path = self.dataset_path / conversation_dir\n",
        "\n",
        "    self.transform = transform\n",
        "\n",
        "    with open(self.text_path, 'r') as f:\n",
        "        self.texts = f.read().splitlines()\n",
        "\n",
        "    with open(self.sentiment_path, 'r') as f:\n",
        "        self.sentiments = np.array(f.read().splitlines()).astype(\"int32\")\n",
        "\n",
        "    with open(self.conversations_path, 'r') as f:\n",
        "        self.conversations = np.array(f.read().splitlines())\n",
        "    \n",
        "  def __len__(self):\n",
        "        return len(self.sentiments)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        img_path = self.images_path / f'{idx}.jpg'\n",
        "        image = Image.open(img_path)\n",
        "        # image = read_image(str(img_path))\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "       \n",
        "        txt = self.texts[idx].strip()\n",
        "        \n",
        "        sentiment = self.sentiments[idx]\n",
        "\n",
        "        data_dict = {\"text\":txt,\n",
        "                     \"image\":image,\n",
        "                     \"sentiment\":sentiment}\n",
        "        return data_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Text_MSCTD(MSCTD_Dataset):\n",
        "    def __init__(self, dataset_dir, conversation_dir, texts, sentiments,\n",
        "                preprocess_func=None, pad_idx=None, max_len=None, transform=None, images_dir=''):\n",
        "        super().__init__(dataset_dir, images_dir, conversation_dir, texts, sentiments, transform)\n",
        "        self.preprocess_func = preprocess_func\n",
        "        self.pad_idx = pad_idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        if self.preprocess_func is not None:\n",
        "            text = self.preprocess_func(text)\n",
        "            if self.max_len is not None:\n",
        "                text = text[:self.max_len]\n",
        "            if self.pad_idx is not None:\n",
        "                text = F.pad(torch.tensor(text), (0, self.max_len - len(text)), 'constant', self.pad_idx)\n",
        "        labels = self.sentiments[idx]\n",
        "        return text, labels"
      ],
      "metadata": {
        "id": "1TidM3mbdPB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_preprocess(text):\n",
        "    return tokenizer.encode_plus(\n",
        "        text,\n",
        "        max_length=MAX_LEN,\n",
        "        truncation=True,\n",
        "        add_special_tokens=True,\n",
        "        return_token_type_ids=True,\n",
        "        return_attention_mask=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "\n",
        "\n",
        "train_dataset_text = Text_MSCTD('drive/MyDrive/dataset/train', 'image_index_train.txt', 'english_train.txt', 'sentiment_train.txt', preprocess_func=bert_preprocess)\n",
        "dev_dataset_text = Text_MSCTD('drive/MyDrive/dataset/dev', 'image_index_dev.txt', 'english_dev.txt', 'sentiment_dev.txt', preprocess_func=bert_preprocess)\n",
        "test_dataset_text = Text_MSCTD('drive/MyDrive/dataset/test', 'image_index_test.txt', 'english_test.txt', 'sentiment_test.txt' , preprocess_func=bert_preprocess)\n",
        "train_loader_text = torch.utils.data.DataLoader(train_dataset_text, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
        "dev_loader_text = torch.utils.data.DataLoader(dev_dataset_text, batch_size=VALID_BATCH_SIZE, shuffle=False)\n",
        "test_loader_text = torch.utils.data.DataLoader(test_dataset_text, batch_size=TEST_BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "iWcgAF0gdjNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb4ZeNL2jbHl"
      },
      "outputs": [],
      "source": [
        "class MultiModalDataset (Dataset):\n",
        "  def __init__(self, dataset_dir, images_dir, texts, transform, preprocess_func=None, pad_idx=None, max_len=None):\n",
        "    self.dataset_path = Path(dataset_dir)\n",
        "    self.images_path = self.dataset_path / images_dir\n",
        "    self.text_path = self.dataset_path / texts\n",
        "    self.transform = transform\n",
        "\n",
        "    self.preprocess_func = preprocess_func\n",
        "    self.pad_idx = pad_idx\n",
        "    self.max_len = max_len\n",
        "\n",
        "    self.df = pd.read_csv(self.text_path, delimiter='|')\n",
        "    self.df.columns = [col.strip() for col in self.df.columns]\n",
        "    # shuffle\n",
        "    self.df = self.df.sample(frac=1).reset_index()\n",
        "    \n",
        "  def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        img_path = self.images_path / self.df.loc[idx, 'image_name']\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "       \n",
        "        text = self.df.loc[idx, 'comment']\n",
        "\n",
        "        if self.preprocess_func is not None:\n",
        "            text = self.preprocess_func(text)\n",
        "            if self.max_len is not None:\n",
        "                text = text[:self.max_len]\n",
        "            if self.pad_idx is not None:\n",
        "                text = F.pad(torch.tensor(text), (0, self.max_len - len(text)), 'constant', self.pad_idx)\n",
        "        \n",
        "        return image, text\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_SlhAlCNkvh",
        "outputId": "1970a915-dc0c-4ba7-fd00-b2fa336eba43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DUyU7pZDjhR"
      },
      "outputs": [],
      "source": [
        "from transformers import BertConfig, BertTokenizer, BertForSequenceClassification\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_l6bH-QDyTp"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
        "config = BertConfig.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YE8KOvSsDzw5"
      },
      "outputs": [],
      "source": [
        "def bert_preprocess(text):\n",
        "    # text = sent_preprocess(text)\n",
        "    return tokenizer.encode_plus(\n",
        "        text,\n",
        "        max_length=MAX_LEN,\n",
        "        truncation=True,\n",
        "        add_special_tokens=True,\n",
        "        return_token_type_ids=True,\n",
        "        return_attention_mask=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "transform = transforms.Compose([transforms.ToTensor()\n",
        "                                ,transforms.Resize((288,288),transforms.InterpolationMode(\"bicubic\"))\n",
        "                                ,transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Mbvp0XQFYQC"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset = MultiModalDataset('./flickr30k_images', 'flickr30k_images', 'results.csv',transform, preprocess_func=bert_preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xm9zqXqtF4qa",
        "outputId": "ca06c995-c86f-413a-fd66-5431c3a0edb6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "158915"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VOKh6k1F5zA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab8e470e-4b84-40fb-b9fe-4d31cd2cbeec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.9043e+00,  1.8259e+00,  1.8604e+00,  ...,  4.0791e-01,\n",
              "           1.7750e-01,  1.6765e-03],\n",
              "         [ 1.8969e+00,  1.8619e+00,  1.8334e+00,  ...,  3.3306e-01,\n",
              "           2.2362e-01, -9.2489e-03],\n",
              "         [ 1.8762e+00,  1.8059e+00,  1.8583e+00,  ...,  2.9619e-01,\n",
              "           1.7266e-01, -2.1515e-02],\n",
              "         ...,\n",
              "         [ 1.5173e+00,  1.2683e+00,  7.9840e-01,  ..., -6.2621e-01,\n",
              "          -6.4593e-01, -6.6441e-01],\n",
              "         [ 1.2687e+00,  3.4323e-01, -2.7090e-02,  ..., -7.1653e-01,\n",
              "          -7.2369e-01, -6.9716e-01],\n",
              "         [ 2.2270e-01,  2.2197e-02,  4.5332e-01,  ..., -7.6719e-01,\n",
              "          -7.6347e-01, -7.4094e-01]],\n",
              "\n",
              "        [[ 2.0902e+00,  2.0695e+00,  2.1014e+00,  ...,  7.2138e-01,\n",
              "           5.2145e-01,  3.2993e-01],\n",
              "         [ 2.0943e+00,  2.1055e+00,  2.0762e+00,  ...,  6.9935e-01,\n",
              "           5.7169e-01,  3.1854e-01],\n",
              "         [ 2.1234e+00,  2.0493e+00,  2.0864e+00,  ...,  7.1900e-01,\n",
              "           5.1827e-01,  3.0891e-01],\n",
              "         ...,\n",
              "         [ 1.8016e+00,  1.5383e+00,  1.0004e+00,  ..., -4.0398e-01,\n",
              "          -4.4597e-01, -5.1335e-01],\n",
              "         [ 1.5527e+00,  5.6720e-01,  1.1708e-01,  ..., -5.2438e-01,\n",
              "          -5.3520e-01, -5.4862e-01],\n",
              "         [ 4.9443e-01,  2.0145e-01,  5.5678e-01,  ..., -6.2726e-01,\n",
              "          -5.8062e-01, -5.8134e-01]],\n",
              "\n",
              "        [[ 2.3378e+00,  2.3357e+00,  2.3666e+00,  ...,  9.2590e-01,\n",
              "           7.0876e-01,  4.7873e-01],\n",
              "         [ 2.3578e+00,  2.3696e+00,  2.3407e+00,  ...,  8.8091e-01,\n",
              "           7.6329e-01,  4.6867e-01],\n",
              "         [ 2.3890e+00,  2.3135e+00,  2.3559e+00,  ...,  8.4983e-01,\n",
              "           6.9824e-01,  4.4230e-01],\n",
              "         ...,\n",
              "         [ 1.9852e+00,  1.8036e+00,  1.0861e+00,  ..., -1.1335e-01,\n",
              "          -1.5709e-01, -2.2439e-01],\n",
              "         [ 1.7070e+00,  7.4137e-01,  1.4717e-01,  ..., -2.4637e-01,\n",
              "          -2.5358e-01, -2.8547e-01],\n",
              "         [ 5.1581e-01,  2.6366e-01,  5.3960e-01,  ..., -3.1965e-01,\n",
              "          -3.3150e-01, -3.7352e-01]]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "img, text = dataset[43]\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skenj-frlmfL"
      },
      "source": [
        "## Processing Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz-aqfStmVqS",
        "outputId": "f1ca5bb5-8d90-4f87-8524-f43c726789ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyenchant in /usr/local/lib/python3.8/dist-packages (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyenchant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUgURQMSls4r",
        "outputId": "f3268ba9-1c42-4119-be63-ff3d8ea3ab34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "import enchant\n",
        "english_dict = enchant.Dict(\"en_US\")\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppuNQUs6nSiB",
        "outputId": "0f989684-22bf-409f-8d88-87536460ff39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.8/dist-packages (2.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji\n",
        "\n",
        "import emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgg-gK_fluWK"
      },
      "outputs": [],
      "source": [
        "NUM = '<NUM>'\n",
        "UNK = '<UNK>'\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def sent_preprocess(sent, lower=True, remove_punct=True, remove_stopwords=False,\n",
        "                    lemmatize=False, handle_nums=False, handle_unknowns=False, remove_emojies=True, join=True):\n",
        "    if lower:\n",
        "        sent = sent.lower()\n",
        "    \n",
        "    if remove_punct:\n",
        "        sent = sent.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    if remove_emojies:\n",
        "        sent = emoji.replace_emoji(sent)\n",
        "    \n",
        "    word_tokens = word_tokenize(sent)\n",
        "\n",
        "    if remove_stopwords:\n",
        "        word_tokens = [w for w in word_tokens if not w in stop_words]\n",
        "\n",
        "    if lemmatize:\n",
        "        word_tokens = [lemmatizer.lemmatize(w) for w in word_tokens]\n",
        "\n",
        "    if handle_nums:\n",
        "        \n",
        "        def is_number(s):\n",
        "            if s.isdigit():\n",
        "                return True\n",
        "            if s[:-2].isdigit():\n",
        "                if s[-2:] == 'th' or s[-2:] == 'st' or s[-2:] == 'nd' or s[-2:] == 'rd':\n",
        "                    return True\n",
        "            return False\n",
        "\n",
        "        word_tokens = [NUM if is_number(w) else w for w in word_tokens]\n",
        "\n",
        "    if handle_unknowns:\n",
        "        word_tokens = [w if english_dict.check(w) else UNK for w in word_tokens]\n",
        "\n",
        "    if join:\n",
        "        return ' '.join(word_tokens)\n",
        "\n",
        "    return word_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6agKdufPDkgY"
      },
      "source": [
        "## Bert Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4nGKvV-CNta",
        "outputId": "31a1d26e-3b17-4dc3-d32f-9dfb6c8831ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(47674, 95349, 95349)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "train_size = int(0.3 * len(dataset))\n",
        "val_size = int(0.6 * len(dataset))\n",
        "test_size = int(0.6 * len(dataset))\n",
        "train_size, val_size, test_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../.."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Il2Y0jHwXtdK",
        "outputId": "8d28e110-952f-4c44-fed2-9faef7966a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, val_set, test_set, _ = torch.utils.data.random_split(dataset, [train_size, val_size, test_size, len(dataset) - train_size - val_size - test_size])\n",
        "     \n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=60, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=60, shuffle=False)\n",
        "test_loader = DataLoader(test_set, batch_size=60, shuffle=False)"
      ],
      "metadata": {
        "id": "0pgd5Cz4kVaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EML7NAvcB8Wz"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veRkvTFXC_mi",
        "outputId": "546f9d0b-7062-424e-975f-98c61cbbdd9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.6.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.25.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --no-cache-dir gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLxTt_20Jp1m"
      },
      "outputs": [],
      "source": [
        "# !gdown 11caq-CNLP6_V3106zj0zAkFyYPAjyjyw\n",
        "#!gdown 14Nh5c1pfzAuLF2nC3-IpugRi7eAMZvhW\n",
        "\n",
        "\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "def load_pretrained_bert(name='drive/MyDrive/models/bert_model.pt'):\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "        num_labels = 3,\n",
        "        output_attentions = False,\n",
        "        output_hidden_states = False,\n",
        "    )\n",
        "    model.load_state_dict(torch.load(name))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_QTVvEpYXnA"
      },
      "outputs": [],
      "source": [
        "class lastLayer(nn.Module):\n",
        "    def __init__(self, pretrained):\n",
        "        super(lastLayer, self).__init__()\n",
        "        self.pretrained = pretrained\n",
        "        self.last = nn.Sequential(\n",
        "            nn.Dropout(p = 0.2,inplace=True),\n",
        "            nn.Linear(1408, 90),\n",
        "            nn.Dropout(p = 0.3,inplace=True),\n",
        "            nn.Linear(90, 30),\n",
        "            nn.Dropout(p = 0.1,inplace=True),\n",
        "            nn.Linear(30, 3),\n",
        "            )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.pretrained(x)\n",
        "        x = self.last(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPNCE5PQJz1l"
      },
      "outputs": [],
      "source": [
        "#!gdown 1EfyDFNxAHGjvnLPRbP0SfnkV9g33-CFJ\n",
        "\n",
        "from torchvision.models import efficientnet_b2, EfficientNet_B2_Weights\n",
        "\n",
        "def load_pretrained_image(name ='drive/MyDrive/models/scene_modal_en.pth'):\n",
        "    image_model = efficientnet_b2(weights=EfficientNet_B2_Weights.IMAGENET1K_V1)\n",
        "    image_model.classifier = nn.Sequential()\n",
        "    image_model = lastLayer(image_model)\n",
        "    image_model.load_state_dict(torch.load(name))\n",
        "    return image_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N1COurhEnEH"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-H8dgN1KRUm"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZm-wK6kEo5j"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "\n",
        "def one_epoch(models, loader, criterion, optimizer=None, epoch='', train=True, set_name='Train', metrics=None):\n",
        "    tmp_accs = []\n",
        "    total_loss = 0\n",
        "    N = len(loader.dataset)\n",
        "    Y = []\n",
        "    Y_pred = []\n",
        "    text_model, image_model = models\n",
        "    text_model.eval()\n",
        "    if train:\n",
        "        image_model.train()\n",
        "    else:\n",
        "        image_model.eval()\n",
        "\n",
        "    with torch.set_grad_enabled(train), tqdm.tqdm(enumerate(loader), total=len(loader)) as pbar:\n",
        "        for i, data_i in pbar:\n",
        "            if train:\n",
        "                optimizer.zero_grad()\n",
        "            image, text = data_i\n",
        "            image = image.to(device)\n",
        "            (input_ids, attention_mask, token_type_ids) = text.values()\n",
        "            input_ids, attention_mask, token_type_ids = input_ids.to(device), attention_mask.to(device), token_type_ids.to(device)\n",
        "            input_ids = input_ids.squeeze(1)\n",
        "            attention_mask = attention_mask.squeeze(1)\n",
        "            token_type_ids = token_type_ids.squeeze(1)\n",
        "            \n",
        "            output = text_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "            p_text = output.logits\n",
        "            p_image = image_model(image)\n",
        "            y_pred = p_image.argmax(dim=-1)\n",
        "            y_true = p_text.argmax(dim=-1)\n",
        "            loss = criterion(p_text, p_image, y_true)\n",
        "            tmp_accs.append(np.count_nonzero(y_pred.cpu()==y_true.cpu()) / len(y_true.cpu()))\n",
        "            total_loss += loss.item() * len(image)\n",
        "            pbar.set_description(f'{epoch}: {set_name} Loss: {total_loss / (i+1):.3e} - Accuracy: {sum(tmp_accs)*100/len(tmp_accs):.2f}%')\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            Y.append(y_true.cpu().numpy())\n",
        "            Y_pred.append(y_pred.cpu().numpy())\n",
        "\n",
        "    total_loss /= N\n",
        "\n",
        "    Y = np.concatenate(Y)\n",
        "    Y_pred = np.concatenate(Y_pred)\n",
        "    acc = accuracy_score(Y_pred, Y)\n",
        "    print(f'Accuracy of {set_name} set: {acc}')\n",
        "\n",
        "    result = {'loss': total_loss, 'accuracy': acc}\n",
        "    if metrics is not None:\n",
        "        result.update({metric: metric_func(Y, Y_pred) for metric, metric_func in metrics.items()})\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44zG8FpkEp3B"
      },
      "outputs": [],
      "source": [
        "def train_model(models, dataloaders, num_epochs, criterion, optimizer, model_name='pytroch-model', scheduler=None):\n",
        "    train_loader, val_loader = dataloaders\n",
        "    min_val_loss = np.inf\n",
        "    min_val_acc = 0\n",
        "\n",
        "    text_model, image_model = models\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        result = one_epoch(models, train_loader, criterion, optimizer, epoch, train=True, set_name='Train')\n",
        "        train_loss = result['loss']\n",
        "        train_acc = result['accuracy']\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_result = one_epoch(models, val_loader, criterion, epoch=epoch, train=False, set_name='Validation')\n",
        "        val_loss = val_result['loss']\n",
        "        val_acc = val_result['accuracy']\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "        \n",
        "        print('\\n', '-' * 60)\n",
        "\n",
        "        if val_acc > max_val_acc:\n",
        "            max_val_acc = val_acc\n",
        "            torch.save(image_model.state_dict(), f'{model_name}.pt')\n",
        "\n",
        "        if val_loss < min_val_loss:\n",
        "            min_val_loss = val_loss\n",
        "\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "    plt.plot(train_losses, label='train')\n",
        "    plt.plot(val_losses, label='val')\n",
        "    plt.title('loss history of training and val sets')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(train_accuracies, label='train')\n",
        "    plt.plot(val_accuracies, label='val')\n",
        "    plt.title('Accuracy history of training and val sets')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    image_model.load_state_dict(torch.load(f'{model_name}.pt'))\n",
        "    return image_model, min_val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFNoeSZhLJ4F"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "\n",
        "def retrain(model, loader, criterion, optimizer=None, epoch='', train=True, model_name='bert', set_name='Train', metrics=None):\n",
        "    total_loss = 0\n",
        "    N = len(loader.dataset)\n",
        "    Y = []\n",
        "    Y_pred = []\n",
        "    model.train()\n",
        "    tmp_accs = []\n",
        "    with torch.set_grad_enabled(train), tqdm.tqdm(enumerate(loader), total=len(loader)) as pbar:\n",
        "        for i, data_i in pbar:\n",
        "            optimizer.zero_grad()\n",
        "            data_i, y = data_i\n",
        "            (input_ids, attention_mask, token_type_ids) = data_i.values()\n",
        "            input_ids, attention_mask, token_type_ids = input_ids.to(device), attention_mask.to(device), token_type_ids.to(device)\n",
        "            y = y.long().to(device)\n",
        "            input_ids = input_ids.squeeze(1)\n",
        "            attention_mask = attention_mask.squeeze(1)\n",
        "            token_type_ids = token_type_ids.squeeze(1)\n",
        "            \n",
        "            output = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=y)\n",
        "            loss = output.loss\n",
        "            \n",
        "            total_loss += loss.item() * len(input_ids)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            y_pred = output.logits.argmax(dim=-1)\n",
        "            \n",
        "            Y.append(y.cpu().numpy())\n",
        "            Y_pred.append(y_pred.cpu().numpy())\n",
        "            tmp_accs.append(np.count_nonzero(y_pred.cpu()==y.cpu()) / len(y.cpu()))\n",
        "            pbar.set_description(f'{epoch}: {set_name} Loss: {total_loss / (i+1):.3e} - Accuracy: {sum(tmp_accs)*100/len(tmp_accs):.2f}%')\n",
        "    total_loss /= N\n",
        "\n",
        "    Y = np.concatenate(Y)\n",
        "    Y_pred = np.concatenate(Y_pred)\n",
        "    acc = accuracy_score(Y_pred, Y)\n",
        "    print(f'Accuracy of {set_name} set: {acc}')\n",
        "\n",
        "    result = {'loss': total_loss, 'accuracy': acc}\n",
        "    if metrics is not None:\n",
        "        result.update({metric: metric_func(Y, Y_pred) for metric, metric_func in metrics.items()})\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnsy6i71HVRO"
      },
      "outputs": [],
      "source": [
        "class DistillationLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.9, T=2):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.T = T\n",
        "        self.kl_div = nn.KLDivLoss(log_target=True, reduction=\"batchmean\")\n",
        "        self.cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, p_true, p_pred, y_true):\n",
        "        logit_loss = self.kl_div(F.log_softmax(p_pred / self.T, dim=-1), F.log_softmax(p_true / self.T, dim=-1)) \n",
        "        label_loss = self.cross_entropy(p_pred, y_true)\n",
        "        return (1-self.alpha) * label_loss + self.alpha * self.T * self.T * logit_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBvnuIfhKbOF",
        "outputId": "af78c7cb-253f-43c7-e7cb-5d2efffe9faf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xJzpE6lErDv"
      },
      "outputs": [],
      "source": [
        "text_model = load_pretrained_bert()\n",
        "text_model = text_model.to(device)\n",
        "text_model.requires_grad_(False)\n",
        "text_model.eval()\n",
        "\n",
        "image_model = load_pretrained_image()\n",
        "image_model = image_model.to(device)\n",
        "image_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIgYSfFzIJad"
      },
      "outputs": [],
      "source": [
        "# Training Configuration\n",
        "LEARNING_RATE = 1e-4\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, image_model.parameters()), lr=LEARNING_RATE)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1, verbose=True, factor=0.5)\n",
        "criterion = DistillationLoss(alpha=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWSBt1F1JJgS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e0f7e1c-e4fd-445a-f141-763c00fae09d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Feb 12 05:57:16 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P0    32W /  70W |   1284MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A    364197      C                                    1281MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "import torch, gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "pt_model = None\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZjjUJIcbgfm"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "average_policy = 'macro'\n",
        "metrics = {'accuracy': accuracy_score, 'precision': lambda y1, y2: precision_score(y1, y2, average=average_policy),\n",
        "           'recall': lambda y1, y2: recall_score(y1, y2, average=average_policy),\n",
        "           'f1': lambda y1, y2: f1_score(y1, y2, average=average_policy),\n",
        "           'confusion_matrix': confusion_matrix}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sf22wAnpOkTu"
      },
      "outputs": [],
      "source": [
        "def eval_model(models, loader, metrics=metrics, set_name='Test', plot_confusion_matrix=True):\n",
        "    results = one_epoch(models, loader, criterion, train=False, set_name=set_name, metrics=metrics)\n",
        "    disp = ConfusionMatrixDisplay(results.pop('confusion_matrix'))\n",
        "    if plot_confusion_matrix:\n",
        "        disp.plot()\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZohg4w_82Fd"
      },
      "source": [
        "### Before training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "dl19-pSNblXF",
        "outputId": "8b189571-e3ec-498d-b548-a88ff08f4b83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ": Test Loss: 2.086e+01 - Accuracy: 29.66%: 100%|██████████| 497/497 [05:01<00:00,  1.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Test set: 0.29669015857034986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': 0.6524287383321729,\n",
              " 'accuracy': 0.29669015857034986,\n",
              " 'precision': 0.35245478034579286,\n",
              " 'recall': 0.3593990727967067,\n",
              " 'f1': 0.2599917007854025}"
            ]
          },
          "metadata": {},
          "execution_count": 97
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "eval_model((text_model, image_model), test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 1e-4\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, image_model.parameters()), lr=LEARNING_RATE)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1, verbose=True, factor=0.5)\n",
        "criterion = DistillationLoss(alpha=0.5)"
      ],
      "metadata": {
        "id": "1mKEgrPDDje-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "fZj1rL6uonvQ",
        "outputId": "e2910f15-800a-4af5-8d17-32e06fb9647f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: Train Loss: 3.502e+01 - Accuracy: 47.65%: 100%|██████████| 795/795 [21:02<00:00,  1.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Train set: 0.47648613500020975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: Validation Loss: 3.465e+01 - Accuracy: 48.34%:  24%|██▍       | 389/1590 [06:42<20:43,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-37375e28e1f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEPOCH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimage_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weakly_sup'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-52-797f19d24055>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(models, dataloaders, num_epochs, criterion, optimizer, model_name, scheduler)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mval_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-3f98e4b86928>\u001b[0m in \u001b[0;36mone_epoch\u001b[0;34m(models, loader, criterion, optimizer, epoch, train, set_name, metrics)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mp_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mp_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-aadb928cc12d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/efficientnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_res_connect\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstochastic_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "EPOCH = 10\n",
        "image_model, min_val_loss = train_model((text_model, image_model), (train_loader, val_loader), EPOCH, criterion, optimizer, model_name='weakly_sup', scheduler=scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avyI3K9eW9Nt",
        "outputId": "85d7270e-b396-4e65-f60e-314ba327951c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  flickr30k_images  flickr-image-dataset.zip  kaggle.json\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGNGm38iXVhY",
        "outputId": "16b9b218-3d46-45e3-d4bc-028b97b9f896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1_9oDyIAaO6cmT08TeUA0pmrbpLYryxsJ/models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(image_model.state_dict(), 'weak_sup.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "sWFdFkX7WuwE",
        "outputId": "91486627-2a03-4f76-9881-265d9b32ea7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-8cc0ec19bd8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weak_sup.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_writer_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_zipfile_writer_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: File weak_sup.pt cannot be opened."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDny095G_b8e"
      },
      "outputs": [],
      "source": [
        "eval_model((text_model, image_model), test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-38c3qqvVUx8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd content/drive/MyDrive\n",
        "torch.save(text_model.state_dict(), 'models/bert_model_weak_sup.pt')\n",
        "torch.save(image_model.state_dict(), 'models/scene_model_weak_sup.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}